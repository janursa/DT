{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "680d82f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import sys\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "import graphviz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9f585355",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df_train, df_test = train_test_split(df,test_size = 0.2,shuffle=False)\n",
    "feature_names = ['OverallQual', 'GrLivArea', 'GarageCars']\n",
    "X_train = df_train[feature_names]\n",
    "y_train = df_train['SalePrice']\n",
    "y_train.to_csv('y_train.csv')\n",
    "\n",
    "X_test = df_test[feature_names]\n",
    "y_test = df_test[['SalePrice']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8fa220e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "18123e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk dt scores train: 0.7793135243935883\n",
      "sk dt scores test: 0.6410367361839437\n"
     ]
    }
   ],
   "source": [
    "## build sklearn regression trree\n",
    "tree_max_depth = 5 # controls overfitting \n",
    "min_samples_leaf = 5 # controls overfitting \n",
    "\n",
    "sk_dt_reg = tree.DecisionTreeRegressor(max_depth = tree_max_depth, min_samples_leaf = min_samples_leaf, criterion = 'squared_error')\n",
    "sk_dt_reg.fit(X_train,y_train)\n",
    "sk_dt_preds = sk_dt_reg.predict(X_train)\n",
    "sk_dt_scores = r2_score(sk_dt_preds,y_train)\n",
    "print('sk dt scores train:',sk_dt_scores)\n",
    "sk_dt_preds = sk_dt_reg.predict(X_test)\n",
    "sk_dt_scores = r2_score(sk_dt_preds,y_test)\n",
    "print('sk dt scores test:',sk_dt_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "62babee3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of feature_names, 3 does not match number of features, 30",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [61]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m### plot the tree\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m dot_data \u001b[38;5;241m=\u001b[39m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport_graphviz\u001b[49m\u001b[43m(\u001b[49m\u001b[43msk_dt_reg\u001b[49m\u001b[43m,\u001b[49m\u001b[43mout_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilled\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspecial_characters\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m graph \u001b[38;5;241m=\u001b[39m graphviz\u001b[38;5;241m.\u001b[39mSource(dot_data)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\tree\\_export.py:889\u001b[0m, in \u001b[0;36mexport_graphviz\u001b[1;34m(decision_tree, out_file, max_depth, feature_names, class_names, label, filled, leaves_parallel, impurity, node_ids, proportion, rotate, rounded, special_characters, precision, fontname)\u001b[0m\n\u001b[0;32m    870\u001b[0m     out_file \u001b[38;5;241m=\u001b[39m StringIO()\n\u001b[0;32m    872\u001b[0m exporter \u001b[38;5;241m=\u001b[39m _DOTTreeExporter(\n\u001b[0;32m    873\u001b[0m     out_file\u001b[38;5;241m=\u001b[39mout_file,\n\u001b[0;32m    874\u001b[0m     max_depth\u001b[38;5;241m=\u001b[39mmax_depth,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    887\u001b[0m     fontname\u001b[38;5;241m=\u001b[39mfontname,\n\u001b[0;32m    888\u001b[0m )\n\u001b[1;32m--> 889\u001b[0m \u001b[43mexporter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecision_tree\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_string:\n\u001b[0;32m    892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exporter\u001b[38;5;241m.\u001b[39mout_file\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\tree\\_export.py:452\u001b[0m, in \u001b[0;36m_DOTTreeExporter.export\u001b[1;34m(self, decision_tree)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_names) \u001b[38;5;241m!=\u001b[39m decision_tree\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 452\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    453\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of feature_names, \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m does not match number of features, \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    454\u001b[0m             \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_names), decision_tree\u001b[38;5;241m.\u001b[39mn_features_in_)\n\u001b[0;32m    455\u001b[0m         )\n\u001b[0;32m    456\u001b[0m \u001b[38;5;66;03m# each part writes to out_file\u001b[39;00m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mValueError\u001b[0m: Length of feature_names, 3 does not match number of features, 30"
     ]
    }
   ],
   "source": [
    "### plot the tree\n",
    "dot_data = tree.export_graphviz(sk_dt_reg,out_file = None,feature_names = feature_names, filled = True, special_characters = True,max_depth=3)\n",
    "graph = graphviz.Source(dot_data)\n",
    "# graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "50cc5a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk dt scores train: 0.9756206622686743\n",
      "sk dt scores test: 0.8408906189026994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "## build sklearn random forest\n",
    "n_estimators = 100\n",
    "criterion='squared_error'\n",
    "max_depth = None\n",
    "min_samples_split = 2\n",
    "min_samples_leaf = 1\n",
    "min_weight_fraction_leaf = 0\n",
    "max_features = sqrt(len(feature_names) - 1)/len(feature_names) # according to GENIE3 suggestion\n",
    "max_leaf_nodes = None\n",
    "min_impurity_decrease = 0\n",
    "bootstrap = True\n",
    "oob_score = True  # to use out-of-bag samples to estimate the generalization score (available for bootstrap=True)\n",
    "n_jobs = None # number of jobs in parallel. fit, predict, decision_path, and apply can be done in parallel over the trees\n",
    "random_state=None # controls randomness in bootstrapping as well as drawing features\n",
    "verbose=1 \n",
    "warm_start=False # reuse the slution of the previous call to fit and add more ensembles to the estimator. look up on Glosery\n",
    "ccp_alpha=0 # complexity parameter used for minima cost-complexity pruning. by default, no prunning\n",
    "max_samples = None # if bootstrap is True, the number of samples to draw from the samples. if none, draw X.shape[0]\n",
    "\n",
    "sk_dt_reg = RandomForestRegressor(n_estimators=n_estimators, criterion=criterion, max_depth=max_depth, min_samples_split=min_samples_split,\n",
    "                                  min_samples_leaf=min_samples_leaf, min_weight_fraction_leaf=min_weight_fraction_leaf, \n",
    "                                  max_features=max_features, max_leaf_nodes=max_leaf_nodes, min_impurity_decrease=min_impurity_decrease,\n",
    "                                  bootstrap=bootstrap, oob_score=oob_score, n_jobs=n_jobs, random_state=random_state, verbose=verbose,\n",
    "                                  warm_start=warm_start, ccp_alpha=ccp_alpha, max_samples=max_samples)\n",
    "sk_dt_reg.fit(X_train,y_train)\n",
    "sk_dt_preds = sk_dt_reg.predict(X_train)\n",
    "sk_dt_scores = r2_score(sk_dt_preds,y_train)\n",
    "print('sk dt scores train:',sk_dt_scores)\n",
    "sk_dt_preds = sk_dt_reg.predict(X_test)\n",
    "sk_dt_scores = r2_score(sk_dt_preds,y_test)\n",
    "print('sk dt scores test:',sk_dt_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd96e3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### build sklearn random forest\n",
    "sk_dt_reg = RandomForestRegressor()\n",
    "sk_dt_reg.fit(X_train,y_train)\n",
    "sk_dt_preds = sk_dt_reg.predict(X_train)\n",
    "sk_dt_scores = r2_score(sk_dt_preds,y_train)\n",
    "print('sk dt scores train:',sk_dt_scores)\n",
    "sk_dt_preds = sk_dt_reg.predict(X_test)\n",
    "sk_dt_scores = r2_score(sk_dt_preds,y_test)\n",
    "print('sk dt scores test:',sk_dt_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2a7607",
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                      feature_names=iris.feature_names,  \n",
    "                      class_names=iris.target_names,  \n",
    "                      filled=True, rounded=True,  \n",
    "                      special_characters=True)  \n",
    "graph = graphviz.Source(dot_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d472fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, x, y, idxs, min_leaf=5):\n",
    "        self.x = x \n",
    "        self.y = y\n",
    "        self.idxs = idxs \n",
    "        self.min_leaf = min_leaf\n",
    "        self.row_count = len(idxs)\n",
    "        self.col_count = x.shape[1]\n",
    "        self.val = np.mean(y[idxs]) \n",
    "        self.score = float('inf')\n",
    "        self.find_varsplit()\n",
    "    def find_varsplit(self): #find where to split\n",
    "        for c in range(self.col_count): self.find_better_split(c) # after this, the row and column of split is determined by scoring\n",
    "        if self.is_leaf: return\n",
    "        x = self.split_col\n",
    "        lhs = np.nonzero(x <= self.split)[0]\n",
    "        rhs = np.nonzero(x > self.split)[0]\n",
    "        self.lhs = Node(self.x, self.y, self.idxs[lhs], self.min_leaf)\n",
    "        self.rhs = Node(self.x, self.y, self.idxs[rhs], self.min_leaf)\n",
    "    @property\n",
    "    def split_col(self): return self.x.values[self.idxs,self.var_idx]\n",
    "\n",
    "    @property\n",
    "    def is_leaf(self): return self.score == float('inf') \n",
    "    def find_better_split(self, var_idx): # determines row and column of the split\n",
    "        x = self.x.values[self.idxs, var_idx]\n",
    "        for r in range(self.row_count):\n",
    "            lhs = x <= x[r]\n",
    "            rhs = x > x[r]\n",
    "            if rhs.sum() < self.min_leaf or lhs.sum() < self.min_leaf: continue # prunning\n",
    "\n",
    "            curr_score = self.find_score(lhs, rhs)\n",
    "            if curr_score < self.score: \n",
    "                self.var_idx = var_idx # the chosen column\n",
    "                self.score = curr_score\n",
    "                self.split = x[r] # the row to split\n",
    "\n",
    "    def find_score(self, lhs, rhs):\n",
    "        y = self.y[self.idxs]\n",
    "        lhs_std = y[lhs].std()\n",
    "        rhs_std = y[rhs].std()\n",
    "#         return lhs_std * lhs.sum() + rhs_std * rhs.sum() #score is calculated by: std_l * sum_x_l + std_r*sum_x_r => ??? why sum of ihs? \n",
    "        return lhs_std + rhs_std  #score is calculated by: std_l * sum_x_l + std_r*sum_x_r => ??? why sum of ihs? \n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.array([self.predict_row(xi) for xi in x])\n",
    "\n",
    "    def predict_row(self, xi):\n",
    "        if self.is_leaf: return self.val\n",
    "        node = self.lhs if xi[self.var_idx] <= self.split else self.rhs\n",
    "        return node.predict_row(xi)\n",
    "class DecisionTreeRegressor:\n",
    "  \n",
    "    def fit(self, X, y, min_leaf = 5):\n",
    "        self.dtree = Node(X, y, idxs = np.array(np.arange(len(y))), min_leaf=min_leaf)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.dtree.predict(X.values)\n",
    "regressor = DecisionTreeRegressor().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450384f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = regressor.predict(X)\n",
    "r2_score(y, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367e15cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = regressor.predict(X_test)\n",
    "r2_score(y_test, pred_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
