{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "680d82f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import sys\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "import graphviz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e1302f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df_train, df_test = train_test_split(df,test_size = 0.2,shuffle=False)\n",
    "feature_names = ['OverallQual', 'GrLivArea', 'GarageCars']\n",
    "X_train = df_train[feature_names]\n",
    "y_train = df_train['SalePrice']\n",
    "y_train.to_csv('y_train.csv')\n",
    "\n",
    "X_test = df_test[feature_names]\n",
    "y_test = df_test[['SalePrice']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fede1354",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cf36f7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk dt scores train: 0.7793135243935883\n",
      "sk dt scores test: 0.6410367361839437\n"
     ]
    }
   ],
   "source": [
    "## build sklearn regression trree\n",
    "tree_max_depth = 5 # controls overfitting \n",
    "min_samples_leaf = 5 # controls overfitting \n",
    "\n",
    "sk_dt_reg = tree.DecisionTreeRegressor(max_depth = tree_max_depth, min_samples_leaf = min_samples_leaf, criterion = 'squared_error')\n",
    "sk_dt_reg.fit(X_train,y_train)\n",
    "sk_dt_preds = sk_dt_reg.predict(X_train)\n",
    "sk_dt_scores = r2_score(sk_dt_preds,y_train)\n",
    "print('sk dt scores train:',sk_dt_scores)\n",
    "sk_dt_preds = sk_dt_reg.predict(X_test)\n",
    "sk_dt_scores = r2_score(sk_dt_preds,y_test)\n",
    "print('sk dt scores test:',sk_dt_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e2806b7d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### plot the tree\n",
    "dot_data = tree.export_graphviz(sk_dt_reg,out_file = None,feature_names = feature_names, filled = True, special_characters = True,max_depth=3)\n",
    "graph = graphviz.Source(dot_data)\n",
    "# graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6d6b4fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk dt scores train: 0.9756206622686743\n",
      "sk dt scores test: 0.8408906189026994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "## build sklearn random forest\n",
    "n_estimators = 100\n",
    "criterion='squared_error'\n",
    "max_depth = None\n",
    "min_samples_split = 2\n",
    "min_samples_leaf = 1\n",
    "min_weight_fraction_leaf = 0\n",
    "max_features = sqrt(len(feature_names) - 1)/len(feature_names) # according to GENIE3 suggestion\n",
    "max_leaf_nodes = None\n",
    "min_impurity_decrease = 0\n",
    "bootstrap = True\n",
    "oob_score = True  # to use out-of-bag samples to estimate the generalization score (available for bootstrap=True)\n",
    "n_jobs = None # number of jobs in parallel. fit, predict, decision_path, and apply can be done in parallel over the trees\n",
    "random_state=None # controls randomness in bootstrapping as well as drawing features\n",
    "verbose=1 \n",
    "warm_start=False # reuse the slution of the previous call to fit and add more ensembles to the estimator. look up on Glosery\n",
    "ccp_alpha=0 # complexity parameter used for minima cost-complexity pruning. by default, no prunning\n",
    "max_samples = None # if bootstrap is True, the number of samples to draw from the samples. if none, draw X.shape[0]\n",
    "\n",
    "sk_dt_reg = RandomForestRegressor(n_estimators=n_estimators, criterion=criterion, max_depth=max_depth, min_samples_split=min_samples_split,\n",
    "                                  min_samples_leaf=min_samples_leaf, min_weight_fraction_leaf=min_weight_fraction_leaf, \n",
    "                                  max_features=max_features, max_leaf_nodes=max_leaf_nodes, min_impurity_decrease=min_impurity_decrease,\n",
    "                                  bootstrap=bootstrap, oob_score=oob_score, n_jobs=n_jobs, random_state=random_state, verbose=verbose,\n",
    "                                  warm_start=warm_start, ccp_alpha=ccp_alpha, max_samples=max_samples)\n",
    "sk_dt_reg.fit(X_train,y_train)\n",
    "sk_dt_preds = sk_dt_reg.predict(X_train)\n",
    "sk_dt_scores = r2_score(sk_dt_preds,y_train)\n",
    "print('sk dt scores train:',sk_dt_scores)\n",
    "sk_dt_preds = sk_dt_reg.predict(X_test)\n",
    "sk_dt_scores = r2_score(sk_dt_preds,y_test)\n",
    "print('sk dt scores test:',sk_dt_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4fdd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "### build sklearn random forest\n",
    "sk_dt_reg = RandomForestRegressor()\n",
    "sk_dt_reg.fit(X_train,y_train)\n",
    "sk_dt_preds = sk_dt_reg.predict(X_train)\n",
    "sk_dt_scores = r2_score(sk_dt_preds,y_train)\n",
    "print('sk dt scores train:',sk_dt_scores)\n",
    "sk_dt_preds = sk_dt_reg.predict(X_test)\n",
    "sk_dt_scores = r2_score(sk_dt_preds,y_test)\n",
    "print('sk dt scores test:',sk_dt_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72c2ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                      feature_names=iris.feature_names,  \n",
    "                      class_names=iris.target_names,  \n",
    "                      filled=True, rounded=True,  \n",
    "                      special_characters=True)  \n",
    "graph = graphviz.Source(dot_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c0acf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, x, y, idxs, min_leaf=5):\n",
    "        self.x = x \n",
    "        self.y = y\n",
    "        self.idxs = idxs \n",
    "        self.min_leaf = min_leaf\n",
    "        self.row_count = len(idxs)\n",
    "        self.col_count = x.shape[1]\n",
    "        self.val = np.mean(y[idxs]) \n",
    "        self.score = float('inf')\n",
    "        self.find_varsplit()\n",
    "    def find_varsplit(self): #find where to split\n",
    "        for c in range(self.col_count): self.find_better_split(c) # after this, the row and column of split is determined by scoring\n",
    "        if self.is_leaf: return\n",
    "        x = self.split_col\n",
    "        lhs = np.nonzero(x <= self.split)[0]\n",
    "        rhs = np.nonzero(x > self.split)[0]\n",
    "        self.lhs = Node(self.x, self.y, self.idxs[lhs], self.min_leaf)\n",
    "        self.rhs = Node(self.x, self.y, self.idxs[rhs], self.min_leaf)\n",
    "    @property\n",
    "    def split_col(self): return self.x.values[self.idxs,self.var_idx]\n",
    "\n",
    "    @property\n",
    "    def is_leaf(self): return self.score == float('inf') \n",
    "    def find_better_split(self, var_idx): # determines row and column of the split\n",
    "        x = self.x.values[self.idxs, var_idx]\n",
    "        for r in range(self.row_count):\n",
    "            lhs = x <= x[r]\n",
    "            rhs = x > x[r]\n",
    "            if rhs.sum() < self.min_leaf or lhs.sum() < self.min_leaf: continue # prunning\n",
    "\n",
    "            curr_score = self.find_score(lhs, rhs)\n",
    "            if curr_score < self.score: \n",
    "                self.var_idx = var_idx # the chosen column\n",
    "                self.score = curr_score\n",
    "                self.split = x[r] # the row to split\n",
    "\n",
    "    def find_score(self, lhs, rhs):\n",
    "        y = self.y[self.idxs]\n",
    "        lhs_std = y[lhs].std()\n",
    "        rhs_std = y[rhs].std()\n",
    "#         return lhs_std * lhs.sum() + rhs_std * rhs.sum() #score is calculated by: std_l * sum_x_l + std_r*sum_x_r => ??? why sum of ihs? \n",
    "        return lhs_std + rhs_std  #score is calculated by: std_l * sum_x_l + std_r*sum_x_r => ??? why sum of ihs? \n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.array([self.predict_row(xi) for xi in x])\n",
    "\n",
    "    def predict_row(self, xi):\n",
    "        if self.is_leaf: return self.val\n",
    "        node = self.lhs if xi[self.var_idx] <= self.split else self.rhs\n",
    "        return node.predict_row(xi)\n",
    "class DecisionTreeRegressor:\n",
    "  \n",
    "    def fit(self, X, y, min_leaf = 5):\n",
    "        self.dtree = Node(X, y, idxs = np.array(np.arange(len(y))), min_leaf=min_leaf)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.dtree.predict(X.values)\n",
    "regressor = DecisionTreeRegressor().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7eacb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = regressor.predict(X)\n",
    "r2_score(y, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2644fef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = regressor.predict(X_test)\n",
    "r2_score(y_test, pred_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
